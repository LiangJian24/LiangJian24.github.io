---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<style>
  dl {
    margin-top: 1px;
    margin-bottom: 5px; /* è°ƒæ•´è¿™ä¸ªå€¼ä»¥è·å¾—åˆé€‚çš„é—´è· */
    clear: both;
  }

  img {
    display: block;
    margin: 0px 10px 10px 0px; /* å›¾ç‰‡å±…ä¸­ ä¸Šå³ä¸‹å·¦*/ 
    max-width: 100%; /* é™åˆ¶å›¾ç‰‡æœ€å¤§å®½åº¦ */
  }

  hr {
    border: 1px solid #ebebeb; /* è°ƒæ•´åˆ†éš”çº¿çš„é¢œè‰²å’Œæ ·å¼ */
    /* margin: 10px;  */
    clear: both; 
  }


  dl dd {
  color: #; 
  margin-top: 1px; 
  margin-bottom: 1px;
}

  dl dd strong {
  font-weight: bold;
  }


  .publication-title {
    font-weight: bold;
  }

  .image-container {
    display: flex;
    justify-content: center;
    gap: 10px; /* æ§åˆ¶å›¾ç‰‡é—´è· */
    margin: 20px 0;
  }

  .image-container img {
    max-width: 150px; /* æ§åˆ¶æœ€å¤§å®½åº¦ */
    height: auto;
    margin: 0; /* ç§»é™¤åŸæ¥çš„ margin */
  }

  .co-first {
    color: #B02418;
  }
  
</style>

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>


# ğŸ§ About Me

Hi there! My name is Jian Liangï¼ˆæ¢å¥ï¼‰

My research focuses on **Mutimodal Large Language Models and Parameter-Efficient Fine-Tuning**.




# ğŸ”¥ News
<div style="max-height: 200px; overflow-y: auto;">
<ul>
  <li><em>2025.02:</em> ğŸš€ LoRASculpt was accepted to <strong>CVPR 2025</strong>.</li>
</ul>
</div>

# ğŸ“ Publications 

&dagger;: equal contribution, * : corresponding author

<hr>

<dl>
  <dt><img align="left" width="400" src="../images/paper/LoRASculpt.png" alt="LVLM_Safety_Survey"></dt>
  <dd><a href="" class="publication-title">LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models</a></dd>
  <dd><strong>Jian Liang</strong>, Wenke Huang, Guancheng Wan, Qu yang, Mang Ye*</dd>
  <dd>Conference on Computer Vision and Pattern Recognition **(CVPR)**, 2025</dd>
</dl>

<hr>

## âŒ›ï¸ In Submission & Preprint
<hr>

<dl>
  <dt><img align="left" width="400" src="../images/paper/MLLMFT_Survey.png" alt="Client As Navigator"></dt>
  <dd><a href="https://arxiv.org/abs/2503.04543" class="publication-title">Keeping Yourself is Important in Downstream Tuning Multimodal Large Language Model</a></dd>
  <dd>Wenke Huang&dagger;, <strong>Jian Liang&dagger;</strong> <span class="co-first">(co-first)</span>, Xianda Guo, Yiyang Fang, Guancheng Wan, Xuankun Rong, Chi Wen, Zekun Shi, Qingyun Li, Didi Zhu, Yanbiao Ma, Ke Liang, Bin Yang, He Li, Jiawei Shao, Mang Ye*, Bo Du*</dd>
  <dd>Under Review</dd>
</dl>

<hr>

<dl>
  <dt><img align="left" width="400" src="../images/paper/SPIDER.png" alt="MLLM_Finetune_Survey"></dt>
  <dd><a href="https://arxiv.org/abs/2411.10928" class="publication-title">Learn from Downstream and Be Yourself in Multimodal Large Language Model Fine-Tuning</a></dd>

  <dd>Wenke Huang, <strong>Jian Liang</strong>, Zekun Shi, Didi Zhu, Guancheng Wan, He Li, Bo Du, Dacheng Tao, Mang Ye*</dd>
  <dd>Under Review</dd>
</dl>

<hr>


# ğŸ– Honors and Awards


# ğŸ“– Educations

